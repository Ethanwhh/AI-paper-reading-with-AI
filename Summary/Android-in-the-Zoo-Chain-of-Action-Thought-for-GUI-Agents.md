# Android in the Zoo: Chain-of-Action-Thought for GUI Agents

## 思维导图
![思维导图](/imgs/Android-in-the-Zoo-Chain-of-Action-Thought-for-GUI-Agents.jpg)

## 全文总结
本文介绍了一种用于提升图形用户界面（GUI）智能体导航能力的方法 Chain-of-Action-Thought（CoAT），以及基于此构建的数据集 Android-In-The-Zoo（AITZ）。
### 1.研究背景与动机
#### 智能手机操作需求：
智能手机在日常生活中至关重要，人们期望通过自然语言指令自动操作 GUI 来简化日常事务，尤其对残疾人意义重大。
#### 现有研究不足：
尽管已有研究探索利用大语言模型（LLM）实现 GUI 导航，但存在问题，如对智能手机操作底层语义关注不足，以往工作常忽略屏幕上多样操作背后的逻辑，仅关注操作坐标，缺乏对中间结果的解释。
### 2.Chain-of-Action-Thought（CoAT）方法
#### 定义：
CoAT 是一种导航交互动态理解捷径，包含屏幕描述（SD）、动作思考（AT）、下一步动作描述（AD）和动作结果（AR）四个基本组件，各组件携带有用语义，可根据语言模型自由组合。
#### 比较优势：
与标准、Chain-of-Action（CoA）和 Chain-of-Thought（CoT）等提示方法相比，CoAT 能携带更多屏幕和动作语义信息。通过在三个强大的专有模型（GPT - 4V、Gemini - Pro - Vision 和 Qwen - VL - Max）上进行初步实验，发现使用 CoAT 的智能体在动作预测准确性上显著优于 CoA 和 CoT，且 GPT - 4V 表现最佳。
### 3.Android-In-The-Zoo（AITZ）数据集
#### 数据收集
##### 指令采样：
基于 AITW 数据集，根据子集特点采用不同采样策略，以减少冗余和过滤错误案例，最终得到 3461 个独特指令和 7180 个情节，再经人工验证和随机选择，确定 5147 个成功情节及对应的指令。
##### 语义标注：
利用 GPT - 4V 生成屏幕描述、动作思考、下一步动作描述和动作结果的候选答案，由专家检查并修正，确保与截图和黄金动作匹配。
#### 数据集分析：
AITZ 数据集包含 2504 个独特指令和 18643 个屏幕 - 动作对，跨越 70 多个 Android 应用程序，与其他相关数据集相比，具有更丰富的指令和语义信息。屏幕描述大多为 80 - 120 字，动作思考 30 - 70 字，动作结果 20 - 80 字。
### 4.实验设置与结果
#### 基线模型：
采用 CogAgent 和 AUTO - UI 作为基线模型，CogAgent 是基于 LLM 的多模态 GUI 智能体，AUTO - UI 是针对 AITW 数据集的 GUI 导航专用模型。
#### 评估指标：
包括原子指标（计算屏幕动作匹配分数）和情节指标（使用目标进度评估智能体对用户查询的完成程度）。
#### 实验结果
##### 零样本评估：
在 CogAgent 上进行零样本评估，结果表明 CoAT 能显著提升模型性能，且微调小模型 AUTO - UI - base 使用 CoAT 可获得与基于 LLM 的智能体相当的性能。
##### 微调评估：
对 CoAT 各组件进行消融研究，发现先前动作结果与动作思考和动作描述结合可显著提高 AUTO - UI 的动作预测准确性，屏幕描述加入输入虽在某些情况下使动作匹配分数和目标进度略有下降，但整体验证了语义标注的必要性和有效性。同时，AITZ 数据训练能提高训练效率。
##### 定性分析：
对错误案例的分析表明，AUTO - UI 难以判断任务执行进度，而先前动作结果可缓解此问题；CogAgent 未考虑历史信息，导致动作重复无效，加入动作思考可改善。
### 5.研究意义与局限
#### 意义：
CoAT 有助于 GUI 智能体更好地感知、思考和决策，AITZ 数据集为 GUI 导航智能体的训练和评估提供了有效资源，推动了该领域发展。
#### 局限：
不同模型结构和训练数据使比较不够直观，图像分辨率、与 GUI 相关的预训练任务对导航性能的影响尚未深入探究。
