# LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models

## 思维导图
![思维导图](/imgs/LLaVA-NeXT-Interleave-Tackling-Multi-image-Video-and-3D-in-Large-Multimodal-Models.jpg)

## 全文总结

“LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models” 由 Feng Li、Renrui Zhang 等人撰写。文章介绍了 LLaVA-NeXT-Interleave 模型，该模型通过采用交错数据格式统一不同任务，提出新数据集和基准，在多模态任务中取得了领先性能，并展现出跨任务转移等新兴能力。

### 研究背景

- 多模态模型进展：大语言模型（LLMs）通过视觉编码器与视觉语言数据结合，在多模态任务中取得进展，但现有开源大模态模型（LMMs）多关注单图像任务，多图像场景研究较少。

- 交错数据格式潜力：交错图像 - 文本训练数据可使 LMMs 具备多模态上下文学习（ICL）和指令跟随能力，不同视觉场景可由交错多图像格式表示。
  
### 研究方法

1.任务与数据

  - 任务概述：采用交错多图像格式统一多图像、多帧（视频）、多视图（3D）和多补丁（单图像）任务，涵盖多种视觉任务。

  - M4-Instruct 数据集：包含 1177.6K 样本，涉及 14 个任务和 41 个数据集，涵盖多图像、视频、3D 和单图像场景，用于训练模型。

  - LLaVA-Interleave Bench 基准：由 13 个具有挑战性的任务和 17K 实例组成，用于评估模型交错多图像性能，分为域内和域外评估。

2.交错视觉指令调整

  - 基于单图像模型继续训练：采用预训练的 LLaVA-NeXT-Image 作为基础模型，进行交错多图像指令调整。

  - 混合交错数据格式训练：在训练时采用两种图像标记位置格式，增强模型对不同输入格式的适应性。

  - 结合不同数据场景：利用 M4-Instruct 同时对四种不同任务进行指令调整，提高单个任务性能。

### 实验结果

1.评估方案与设置

  - 评估方案：在多图像、视频、3D 和单图像场景中评估模型，采用相应的基准和指标。

  - 实现细节：模型架构采用 Qwen 1.5 作为基础语言模型，SigLIP-400M 作为视觉编码器，两层 MLP 作为投影层。

2.主要结果

  - 多图像场景：在域内和域外基准测试中性能超越之前的开源模型，展示了良好的泛化能力。

  - 多帧（视频）场景：在多个视频基准测试中取得优异成绩，DPO 训练后 7B 模型在 VDD 和 VideoChat-GPT 基准测试中达到 SoTA 性能。

  - 多视图（3D）场景：在五个域内基准测试中，室内外 3D 感知任务均获得领先结果。

  - 多补丁（单图像）场景：保持了单图像性能，单图像数据的加入提高了指令跟随能力和任务转移能力。

3.消融实验

  - 单图像模型初始化：从良好的单图像模型检查点初始化可提升交错多图像性能。

  - 混合交错数据格式：混合格式训练对两种输入格式的结果均有益。

  - 数据场景组合：整合更多数据来源有助于提升视频任务性能。

### 新兴能力

1.任务转移：模型展示了从单图像到多图像、从图像到视频的任务转移能力，如分析多图像中的有趣部分、为视频生成推特帖子。

2.实际应用：在多图像绘画风格识别、PPT 总结与问答、多文档视觉问答等实际场景中表现出泛化潜力。

### 研究结论

LLaVA-NeXT-Interleave 模型通过交错数据格式统一多种视觉任务，在多模态模型能力提升方面具有变革性潜力。M4-Instruct 数据集和 LLaVA-Interleave Bench 为模型训练和评估提供了坚实基础，模型在多图像任务中达到新的 SoTA 水平，同时保持单图像性能，展现出跨任务转移等新兴能力，为多模态人工智能和复杂视觉理解任务的未来发展奠定了基础。
