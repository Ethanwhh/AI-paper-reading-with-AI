# LLaVA-OneVision: Easy Visual Task Transfer

## 思维导图
![思维导图](/imgs/LLaVA-OneVision-Easy-Visual-Task-Transfer.jpg)

## 全文总结

“LLaVA-OneVision: Easy Visual Task Transfer” 由 Bo Li、Yuanhan Zhang、Dong Guo 等作者撰写，提出了 LLaVA-OneVision 这一开放大型多模态模型家族，在单图像、多图像和视频场景中展现出卓越性能，其设计允许跨场景任务转移，催生新能力。

### 1.研究背景与目的

- 背景：构建通用视觉助手是人工智能的核心目标，LLaVA 系列模型不断发展，LLaVA-OneVision 在此基础上整合经验，旨在提升性能。

- 目的：开发能在多种视觉场景下有效执行任务的模型，通过创新设计和训练策略，推动多模态模型发展。

### 2.方法

- 模型架构：采用极简主义设计，包括语言模型（LLM）、视觉编码器和投影器。以 Qwen-2 作为 LLM，SigLIP 作为视觉编码器，2 层 MLP 作为投影器，有效融合视觉和语言能力。

- 视觉表示：提出 Higher AnyRes 策略，根据不同场景（单图像、多图像、视频）灵活配置分辨率和令牌数量，平衡性能与成本，实现跨场景能力转移。

- 数据处理

  - 高质量知识学习：利用模型生成新描述，整合多种数据来源，如重新标注的描述数据、文档 / OCR 数据和中文语言数据，以提升模型知识水平。

  - 视觉指令调整数据：从多个数据源收集数据，按视觉输入、指令和响应进行分类，分为单图像和多视觉场景数据，确保数据高质量和多样性。

- 训练策略：分三个阶段训练模型，包括语言 - 图像对齐、高质量知识学习和视觉指令调整。通过课程学习原则，逐步增加训练难度，提高模型性能。

### 3.实验结果

- 性能评估：在多个基准测试中评估 LLaVA-OneVision 模型，包括单图像、多图像和视频基准。在单图像任务中，如文档理解、感知推理和真实世界理解等方面表现出色；在多图像任务中，如多图像推理和差异识别等方面取得显著进步；在视频任务中，如时空推理和视频理解等方面与先进模型竞争。

- 新兴能力展示：模型通过任务转移展现出多种新兴能力，如联合理解图表、处理 GUI 任务、进行 Set-of-mark Prompting、生成图像到视频编辑指令、分析视频差异、理解多摄像头视频、解读组合子视频、处理视频中的视觉提示和视觉指代等。

### 4.结论：
LLaVA-OneVision 是一种强大的开放多模态模型，通过整合经验和创新设计，在多种视觉场景中表现出色，为构建通用视觉助手奠定基础，未来可通过进一步扩展和优化实现更强性能。






