# A Fast Learning Algorithm for Deep Belief Nets

## 思维导图
![思维导图](/imgs/A-Fast-Learning-Algorithm-for-Deep-Belief-Nets.png)

## 全文总结
本文介绍了一种用于深度信念网络的快速学习算法，通过使用 “互补先验” 来消除推理困难的问题，并提出了一种快速贪心算法和微调算法，用于学习深度有向信念网络。该算法在手写数字图像和标签的联合分布建模中取得了良好的效果，并且在 MNIST 数据库上的分类性能优于其他方法。
### 1.引言
#### 学习困难：
在具有许多隐藏层的密集连接有向信念网络中，由于难以推断给定数据向量时隐藏活动的条件分布，学习变得困难。
#### 本文模型优势：
本文提出的模型，其顶部两层形成无向联想记忆，其余隐藏层形成有向无环图，具有快速贪心学习算法、可应用于标记数据、有微调算法、生成模型易于解释、推理快速准确、学习算法局部且通信简单等优点。
### 2.互补先验
#### 解释消除困难：
“解释消除” 现象使有向信念网络中的推理变得困难，本文提出使用 “互补先验” 来消除该现象，使后验分布变为因子化的，从而简化推理。
#### 无限有向模型示例：
给出了一个具有绑定权重的无限有向模型示例，通过该模型可以从真实后验分布中采样，计算数据的对数概率导数，展示了其与受限玻尔兹曼机（RBM）的等价性。
### 3.受限玻尔兹曼机与对比散度学习：
受限玻尔兹曼机（RBM）与无限有向网络的等价性，以及对比散度学习在 RBM 中的应用，该学习方法在实践中高效但存在一定问题，为多层网络学习算法提供了思路。
### 4.转换表示的贪心学习算法
#### 算法思想：
通过顺序学习一系列简单模型来构建复杂模型，每个模型对输入数据进行非线性转换，并将转换后的输出作为下一个模型的输入，以学习不同的数据表示。
#### 多层生成模型应用：
在多层生成模型中，通过假设高层权重构建互补先验来初始化参数，然后冻结底层权重，学习高层 “数据” 的 RBM 模型，该算法可递归应用，保证生成模型的改进。
### 5.使用上下算法进行反向拟合：
学习每层权重后，使用上下算法进行反向拟合，调整 “识别” 权重以适应 “生成” 权重，该算法是唤醒 - 睡眠算法的对比版本，可提高模型性能。
### 6.在 MNIST 数据库上的性能
#### 训练网络：
使用 MNIST 手写数字数据库进行训练，通过贪心算法和上下算法的结合，网络在测试集上的错误率达到 1.25%，优于许多其他方法。
#### 测试网络：
介绍了两种测试网络的方法，一种是使用随机向上传递和交替吉布斯采样来激活标签单元，另一种是计算精确自由能来确定标签，后一种方法更准确。
### 7.探究神经网络的思维：
通过在顶级联想记忆中进行交替吉布斯采样，可以生成模型的样本，观察网络在不同条件下的内部状态和生成的图像，从而理解网络的学习和决策过程。
### 8.结论：
本文提出的算法能够逐层学习深度信念网络，通过互补先验和贪心算法初始化权重，再用微调算法优化，生成模型具有多种优势，但也存在局限性。未来可能需要更大的网络来与人类形状识别能力竞争，同时也需要改进模型以处理更复杂的图像和任务。
### 附录 A：互补先验
一般互补性：定义了互补先验的概念，即对于给定的似然函数，存在一种先验分布使得联合分布导致的后验分布可以因子化。给出了似然函数和互补先验的具体形式，并证明了两者之间的对应关系。
无限堆叠的互补性：考虑了似然函数也可因子化的模型子集，通过将其与无向图模型的条件独立性联系起来，证明了可以用无限有向模型表示该联合分布，且推理过程在无限图中是精确的。
### 附录 B：上下算法的伪代码
提供了上下算法的 MATLAB 风格伪代码，包括自底向上传递获取唤醒 / 正相概率和采样状态、使用顶级无向联想记忆进行吉布斯采样、自顶向下生成传递获取睡眠 / 负相概率和采样状态，以及更新生成参数、顶级联想记忆参数和识别 / 推理近似参数等步骤。
