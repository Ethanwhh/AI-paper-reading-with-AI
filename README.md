# AI-paper-reading-with-AI

**We all know that reading papers is boringüò¥. In a limited time, we hope to quickly understand an article, grasp the core points in the shortest possible time, and then make trade-offs with accuracy. Therefore, I established a reasonable paper analysis workflow, aiming to quickly extract effective informationüòÉ.**

*The following is the structure of the summary:*

```
1Ô∏è‚É£Mind map
2Ô∏è‚É£Main content
       Author and team information
       Background and motivation
       Related research
       Core idea
       Solutions and technologies
       Experiment and summary
       Main contribution
       Deficiency
3Ô∏è‚É£Code implementation
```

## GUI agent
|üìÑTitle|üë®‚ÄçüíªAuthor|üìëPublisher|üîëKey|üìñSummary(EN / ZH-CN)|üìÖDate|üîóLinks|
|:-----:|:-------:|:---------:|:----:|:-------------------:|:-----:|:-----:|
|GUI Agents: A Survey|Dang Nguyen, Jian Chen, Yu Wang, Gang Wu, Namyong Park, Zhengmian Hu, Hanjia Lyu, Junda Wu, Ryan Aponte, Yu Xia, Xintong Li, Jing Shi, Hongjie Chen, Viet Dac Lai, Zhouhang Xie, Sungchul Kim, Ruiyi Zhang, Tong Yu, Mehrab Tanjim, Nesreen K. Ahmed, Puneet Mathur, Seunghyun Yoon, Lina Yao, Branislav Kveton, Thien Huu Nguyen, Trung Bui, Tianyi Zhou, Ryan A. Rossi, Franck Dernoncourt|arXiv|Survey|EN / ZH-CN|18 Dec 2024|[paper](https://arxiv.org/abs/2412.13501)|

|AgentStudio: A Toolkit for Building General Virtual Agents|Longtao Zheng, Zhiyuan Huang, Zhenghai Xue, Xinrun Wang, Bo An, Shuicheng Yan|[Summary](/Summary/AgentStudio-A-Toolkit-for-Building-General-Virtual-Agents.md)|[paper](https://arxiv.org/abs/2403.17918)|
|Android in the Wild: A Large-Scale Dataset for Android Device Control|Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, Timothy Lillicrap|[Summary](/Summary/Android-in-the-Wild-A-Large-Scale-Dataset-for-Android-Device-Control.md)|[paper](https://arxiv.org/abs/2307.10088)|
|Android in the Zoo: Chain-of-Action-Thought for GUI Agents|Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, Duyu Tang|[Summary](/Summary/Android-in-the-Zoo-Chain-of-Action-Thought-for-GUI-Agents.md)|[paper](https://arxiv.org/abs/2403.02713)|
|AppAgent: Multimodal Agents as Smartphone Users|Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, Gang Yu|[Summary](/Summary/AppAgent-Multimodal-Agents-as-Smartphone-Users.md)|[paper](https://arxiv.org/abs/2312.13771)|
|ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation|Difei Gao, Lei Ji, Zechen Bai, Mingyu Ouyang, Peiran Li, Dongxing Mao, Qinchen Wu, Weichen Zhang, Peiyi Wang, Xiangwu Guo, Hengxu Wang, Luowei Zhou, Mike Zheng Shou|[Summary](Summary/ASSISTGUI-Task-Oriented-Desktop-Graphical-User-Interface-Automation.md)|[paper](https://arxiv.org/abs/2312.13108)|
|CogAgent: A Visual Language Model for GUI Agents|Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, Jie Tang|[Summary](/Summary/CogAgent-A-Visual-Language-Model-for-GUI-Agents.md)|[paper](https://arxiv.org/abs/2312.08914)|
|CRAB: Cross-environment Agent Benchmark for Multimodal Language Model Agents|Tianqi Xu, Linyao Chen, Dai-Jie Wu, Yanjun Chen, Zecheng Zhang, Xiang Yao, Zhiqiang Xie, Yongchao Chen, Shilong Liu, Bochen Qian, Anjie Yang, Zhaoxuan Jin, Jianbo Deng, Philip Torr, Bernard Ghanem, Guohao Li|[Summary](/Summary/CRAB-Cross-environment-Agent-Benchmark-for-Multimodal-Language-Model-Agents.md)|[paper](https://arxiv.org/abs/2407.01511)|
|Cradle: Empowering Foundation Agents Towards General Computer Control|Weihao Tan, Wentao Zhang, Xinrun Xu, Haochong Xia, Ziluo Ding, Boyu Li, Bohan Zhou, Junpeng Yue, Jiechuan Jiang, Yewen Li, Ruyi An, Molei Qin, Chuqiao Zong, Longtao Zheng, Yujie Wu, Xiaoqiang Chai, Yifei Bi, Tianbao Xie, Pengjie Gu, Xiyun Li, Ceyao Zhang, Long Tian, Chaojie Wang, Xinrun Wang, B√∂rje F. Karlsson, Bo An, Shuicheng Yan, Zongqing Lu|[Summary](/Summary/Cradle-Empowering-Foundation-Agents-Towards-General-Computer-Control.md)|[paper](https://arxiv.org/abs/2403.03186)|
|DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning|Hao Bai, Yifei Zhou, Mert Cemri, Jiayi Pan, Alane Suhr, Sergey Levine, Aviral Kumar|[Summary](/Summary/DigiRL-Training-In-The-Wild-Device-Control-Agents-with-Autonomous-Reinforcement-Learning.md)|[paper](https://arxiv.org/abs/2406.11896)|
|Enrico: A Dataset for Topic Modeling of Mobile UI Designs| Luis A. Leiva, Asutosh Hota, Antti Oulasvirta|[Summary](/Summary/Enrico-A-Dataset-for-Topic-Modeling-of-Mobile-UI-Designs.md)|[paper](https://dl.acm.org/doi/10.1145/3406324.3410710)|
|Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs|Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin, Jeffrey Nichols, Yinfei Yang, Zhe Gan|[Summary](/Summary/Ferret-UI-Grounded-Mobile-UI-Understanding-with-Multimodal-LLMs.md)|[paper](https://arxiv.org/abs/2404.05719)|
|Ferret: Refer and Ground Anything Anywhere at Any Granularity|Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, Yinfei Yang|[Summary](Summary/Ferret-Refer-and-Ground-Anything-Anywhere-at-Any-Granularity.md)|[paper](https://arxiv.org/abs/2310.07704)|
|GPT-4V(ision) is a Generalist Web Agent, if Grounded|Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, Yu Su|[Summary](/Summary/GPT-4V(ision)-is-a-Generalist-Web-Agent-if-Grounded.md)|[paper](https://arxiv.org/abs/2401.01614)|
|GUI-WORLD: A Dataset for GUI-oriented Multimodal LLM-based Agents|Dongping Chen, Yue Huang, Siyuan Wu, Jingyu Tang, Liuyi Chen, Yilin Bai, Zhigang He, Chenlong Wang, Huichi Zhou, Yiqiang Li, Tianshuo Zhou, Yue Yu, Chujie Gao, Qihui Zhang, Yi Gui, Zhen Li, Yao Wan, Pan Zhou, Jianfeng Gao, Lichao Sun|[Summary](/Summary/GUI-WORLD-A-Dataset-for-GUI-oriented-Multimodal-LLM-based-Agents.md)|[paper](https://arxiv.org/abs/2406.10819)|
|GUICourse: From General Vision Language Models to Versatile GUI Agents|Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen, Yupeng Huo, Yuan Yao, Yankai Lin, Zhiyuan Liu, Maosong Sun|[Summary](/Summary/GUICourse-From-General-Vision-Language-Models-to-Versatile-GUI-Agents.md)|[paper](https://arxiv.org/abs/2406.11317)|
|LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models|Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, Chunyuan Li|[Summary](/Summary/LLaVA-NeXT-Interleave-Tackling-Multi-image-Video-and-3D-in-Large-Multimodal-Models.md)|[paper](https://arxiv.org/abs/2407.07895)|
|LLaVA-OneVision: Easy Visual Task Transfer|Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li|[Summary](/Summary/LLaVA-OneVision-Easy-Visual-Task-Transfer.md)|[paper](https://arxiv.org/abs/2408.03326)|
|Mapping Natural Language Instructions to Mobile UI Action Sequences|Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, Jason Baldridge|[Summary](/Summary/Mapping-Natural-Language-Instructions-to-Mobile-UI-Action-Sequences.md)|[paper](https://arxiv.org/abs/2005.03776)|
|Mind2Web: Towards a Generalist Agent for the Web|Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, Yu Su|[Summary](/Summary/Mind2Web-Towards-a-Generalist-Agent-for-the-Web.md)|[paper](https://arxiv.org/abs/2306.06070)|
|MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?|Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, Liang Wang, Rong Jin, Tieniu Tan|[Summary](/Summary/MME-RealWorld-Could-Your-Multimodal-LLM-Challenge-High-Resolution-Real-World-Scenarios-that-are-Difficult-for-Humans.md)|[paper](https://arxiv.org/abs/2408.13257)|
|MobileAgentBench: An Efficient and User-Friendly Benchmark for Mobile LLM Agents|Luyuan Wang, Yongyu Deng, Yiwei Zha, Guodong Mao, Qinmin Wang, Tianchen Min, Wei Chen, Shoufa Chen|[Summary](/Summary/MobileAgentBench-An-Efficient-and-User-Friendly-Benchmark-for-Mobile-LLM-Agents.md)|[paper](https://arxiv.org/abs/2406.08184)|
|Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents|Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, Yu Su|[Summary](/Summary/Navigating-the-Digital-World-as-Humans-Do-Universal-Visual-Grounding-for-GUI-Agents.md)|[paper](https://arxiv.org/abs/2410.05243)|
|Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding|Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova|[Summary](/Summary/Pix2Struct-Screenshot-Parsing-as-Pretraining-for-Visual-Language-Understanding.md)|[paper](https://arxiv.org/abs/2210.03347)|
|Rico: A Mobile App Dataset for Building Data-Driven Design Applications|Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman,  Daniel Afergan, Yang Li, Jeffrey Nichols, Ranjitha Kumar|[Summary](/Summary/Rico-A-Mobile-App-Dataset-for-Building-Data-Driven-Design-Applications.md)|[paper](https://dl.acm.org/doi/10.1145/3126594.3126651)|
|Screen Recognition: Creating Accessibility Metadata for Mobile Applications from Pixels|Xiaoyi Zhang, Lilian de Greef, Amanda Swearngin, Samuel White, Kyle Murray, Lisa Yu, Qi Shan, Jeffrey Nichols, Jason Wu, Chris Fleizach, Aaron Everitt, Jeffrey P. Bigham|[Summary](/Summary/Screen-Recognition-Creating-Accessibility-Metadata-for-Mobile-Applications-from-Pixels.md)|[paper](https://arxiv.org/abs/2101.04893)|
|ScreenAI: A Vision-Language Model for UI and Infographics Understanding|Gilles Baechler, Srinivas Sunkara, Maria Wang, Fedir Zubach, Hassan Mansoor, Vincent Etter, Victor CƒÉrbune, Jason Lin, Jindong Chen, Abhanshu Sharma|[Summary](/Summary/ScreenAI-A-Vision-Language-Model-for-UI-and-Infographics-Understanding.md)|[paper](https://arxiv.org/abs/2402.04615)|
|SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents|Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, Zhiyong Wu|[Summary](/Summary/SeeClick-Harnessing-GUI-Grounding-for-Advanced-Visual-GUI-Agents.md)|[paper](https://arxiv.org/abs/2401.10935)|
|Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models|Guanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, Jingren Zhou|[Summary](/Summary/Self-play-with-Execution-Feedback-Improving-Instruction-following-Capabilities-of-Large-Language-Models.md)|[paper](https://arxiv.org/abs/2406.13542)|
|UFO: A UI-Focused Agent for Windows OS Interaction|Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang|[Summary](/Summary/UFO-A-UI-Focused-Agent-for-Windows-OS-Interaction.md)|[paper](https://arxiv.org/abs/2402.07939)|
|VGA: Vision GUI Assistant -- Minimizing Hallucinations through Image-Centric Fine-Tuning|Ziyang Meng, Yu Dai, Zezheng Gong, Shaoxiong Guo, Minglong Tang, Tongquan Wei|[Summary](/Summary/VGA-Vision-GUI-Assistant-Minimizing-Hallucinations-through-Image-Centric-Fine-Tuning.md)|[paper](https://arxiv.org/abs/2406.14056)|
|You Only Look at Screens: Multimodal Chain-of-Action Agents|Zhuosheng Zhang, Aston Zhang|[Summary](/Summary/You-Only-Look-at-Screens-Multimodal-Chain-of-Action-Agents.md)|[paper](https://arxiv.org/abs/2309.11436)

















 
 
